{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yajie-Xu/HuggingFace-Course-NLP/blob/main/course/en/chapter2/section5_pt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-36iklBTZlw"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tUw0B1OTZlx"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrTebWI3TZlx"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models expect a batch of inputs"
      ],
      "metadata": {
        "id": "C5-kklT9VrSK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XEHXXJbTZlx",
        "outputId": "c2548720-3268-4063-b4ce-da8c2d5858aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the tokenizer didn’t just convert the list of input IDs into a tensor, it added a dimension on top of it:"
      ],
      "metadata": {
        "id": "y0-7ko6kVzGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZsfXXsTTZly",
        "outputId": "2cd854f6-e66a-40ce-9af9-fd45bef49086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
              "          2607,  2026,  2878,  2166,  1012,   102]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOtIFQ5PTZly",
        "outputId": "53b35741-6a30-4b7a-ff03-725995119c52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n",
              "Logits: [[-2.7276,  2.8789]]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Batching* is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:"
      ],
      "metadata": {
        "id": "62Emllx2V9wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a batch of two identical sequences!\n",
        "# batched_ids = [ids, ids]"
      ],
      "metadata": {
        "id": "9ehxYasYWGfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ Try it out! Convert this batched_ids list into a tensor and pass it through your model. Check that you obtain the same logits as before (but twice)!"
      ],
      "metadata": {
        "id": "shkwP_97WRM3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "msSXNgP0WINR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding the inputs"
      ],
      "metadata": {
        "id": "lp4RxXGhWe0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. There’s a second issue, though. When you’re trying to batch together two (or more) sentences, they might be of **different lengths**. If you’ve ever worked with tensors before, you know that they need to be of **rectangular shape**, so you won’t be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually pad the inputs.\n"
      ],
      "metadata": {
        "id": "aLrXDXz3WYjN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9MtYNIuTZly"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to work around this, we’ll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the *padding token* to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:\n"
      ],
      "metadata": {
        "id": "e417fBnnWujc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhzo8HPCTZly"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86GeqYM-TZly",
        "outputId": "51abe219-5ce5-4f15-8432-a97dce71d242"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
              "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
              "tensor([[ 1.5694, -1.3895],\n",
              "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Attention masks"
      ],
      "metadata": {
        "id": "BGIpVmm7W-mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THXOsVevTZlz",
        "outputId": "2c042bdf-e162-4f34-fd50-9fdff0eefd16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.5694, -1.3895],\n",
              "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45yhlzdbTZlz"
      },
      "outputs": [],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}