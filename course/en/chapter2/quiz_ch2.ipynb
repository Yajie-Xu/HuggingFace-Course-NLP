{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9d6MOmPtutGHq9lXaNNtU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yajie-Xu/HuggingFace-Course-NLP/blob/main/course/en/chapter2/quiz_ch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2nJepAvIjaO"
      },
      "source": [
        "# Ch2 Quiz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bNV7BkLIjaP"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZPFahPiIjaP"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. What does the result variable contain in this code sample?\n"
      ],
      "metadata": {
        "id": "lbbGlWFpJz74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "result = tokenizer.tokenize(\"Hello!\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "8dd463db0f164eecb724d45635a1e70d",
            "0589c3961eff489a91b8a29dcd834c5e",
            "04acec959f0046f587a4158563a789ba",
            "4c9a830928984ab3a8d9d392d028e9ef",
            "463e8fbaee394e8bb4741f07ca1e8a69",
            "acb7a55a68914500856ddfd6593d41dc",
            "4652d06b641d41ab9f8a56445c2b0b0c",
            "dd2c7c45526c46b68d4a6fb1fd6ffb1d",
            "d1f8804fa27c40ae87b0ff17e34e92be",
            "10fa2ff8866d4140bd2d8b1fc0cc1913",
            "72438747cb6f43ba92acce429bf83e9d",
            "b4422b6f98b549308b5dda4aa831165e",
            "7a9c3cb056d14d5ea4648f7f2aac2be8",
            "61cb34f14cb34e1e9c6ac10c24e61954",
            "1d85b7041a3a467b8e55f57647be34c9",
            "90e3e3b1b42b40aab7d335172dc7faa6",
            "6c0651419102428ea89d8a82bea399e6",
            "e2f7c51f5a4843e988c5642ef097866b",
            "aae38131ac434da1ab2d65d9cf208fb9",
            "daafaf63505f496a854388e40342101a",
            "0820439389a84df09a5a9670ff1f8bc3",
            "05b10283d5354d3680f8e5ae7515bcbf",
            "2528cb027e6946088b093469102f186f",
            "a425da1e9b08409f96c83e03112e46f0",
            "cd9e5b6797b54a35ba3e77860bab46c2",
            "198bdbcf145a4d71babe1f32538f9d3c",
            "6a03540f7f7a4c94a7cc15c224a82b85",
            "1c84f7c04d594cbc93536920bf2708da",
            "13cd18121aa1451884bf6271d81f8398",
            "1b7af090ea7d4c02ac5755b2e2bdd5fb",
            "21f52014252d4d2c9f44fd2cc8ec3ba0",
            "9de48042dd8443bc8edb4e12e7d6f309",
            "2dd8dfa4ccd643a8a19ee6ca97ad97fc",
            "14265cf29dc9416888335f6bf4b17c8c",
            "e65857ade6b24fe09e3d277be3cecb9e",
            "3ad1646dd1214ab1a9af3e8d5518dd58",
            "fda1d65192634fa7be562e36d3d4495c",
            "0e02d7b94b92442f8267584b7fefae64",
            "d795004c421d43a8a4731821f0fa0b45",
            "1b476824b09142198ede24b78d35c53c",
            "834ac6e9cc2549b0a94657e6f3f1f09d",
            "8237e0853a6d4b79bede5652bb271fe3",
            "9c03d0ac6fd2472fba5da07b4f76d86d",
            "4037214c8ec24b92ac52edbbd7c1fa0c"
          ]
        },
        "id": "JlngOpKDI-1h",
        "outputId": "fc54b80f-a926-4449-a1aa-32e5a85a240e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dd463db0f164eecb724d45635a1e70d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4422b6f98b549308b5dda4aa831165e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2528cb027e6946088b093469102f186f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14265cf29dc9416888335f6bf4b17c8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Is there something wrong with the following code?"
      ],
      "metadata": {
        "id": "FRhyq8qEKNWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "model = AutoModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "encoded = tokenizer(\"Hey!\", return_tensors=\"pt\")\n",
        "result = model(**encoded)"
      ],
      "metadata": {
        "id": "Vc2BU_fcKGwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " > A: The tokenizer and model should always be from the same checkpoint."
      ],
      "metadata": {
        "id": "JIdnQL20KSr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Head VS Model Output"
      ],
      "metadata": {
        "id": "K-D0QT1QNO6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clarification:\n",
        "\t1.\tModel Head = A predefined layer(s) at the end of the model\n",
        "\n",
        "1. Model Head = A predefined layer(s) at the end of the model\n",
        "  * It is part of the model architecture.\n",
        "  * It transforms hidden states from the base model into task-specific outputs.\n",
        "  * Example: A linear layer (torch.nn.Linear) for classification.\n",
        "\n",
        "2. Model Output = The actual values computed during inference\n",
        "\t*\tIt is what the model produces after running a forward pass.\n",
        "\t*\tExample: Logits, probabilities, predicted labels, etc.\n",
        "\n",
        "Example: DistilBERT for Sentiment Classification\n",
        "\n",
        "*\tBase Model (distilbert): Extracts hidden representations from input text.\n",
        "*\tModel Head (classifier): A linear layer that maps hidden states to logits.\n",
        "*\tModel Output (logits): The final numerical values returned when you run inference."
      ],
      "metadata": {
        "id": "POpB-OPNNm86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load model and tokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# Tokenize input\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\"]\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Run model\n",
        "output = model(**tokens)\n",
        "\n",
        "# Inspect components\n",
        "print(\"Model Head (classifier layer):\", model.classifier)\n",
        "print(\"Model Output (logits):\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308,
          "referenced_widgets": [
            "a2f9d456759d41d282803e84d8e334b3",
            "4e30f2e7b4b24dee929499852e9baa38",
            "219e178d75f848f285b536cd62ae97c4",
            "e39a6af06dc1422591c656457422b58a",
            "28169ab821d34520b78adf6cb44092bb",
            "2095cc7bb14b402799e8dcab5e1e7ac5",
            "4950b7d8b4d44a188d6affa20550925c",
            "21ebc8f5434a4fd2bd43ac7553f48c9c",
            "2738ce102f76452ca40ba0854a8027b4",
            "3e3dd04af5ba4923b4488aa27127e3c0",
            "b35089969f454c6cb09222f5aa4f359c",
            "89f8474636ce46679e734264609a4bac",
            "c9c230bb92c7471f82c66e662a2e6c7f",
            "1908da041eae43f39b45c9b2fdf7e15d",
            "1d74f252078f4578bac09c84d053871f",
            "375e6f12b7b04a328f03fc074ed9cd93",
            "49235480db4b4c3a87f20eadb0b01423",
            "3edcf3b4c5014bef978405acea85b267",
            "f32de0202903410daec8479914d5a81d",
            "85765f8d864a4a9bb9fe90b5c1233e5e",
            "c894d48db73c42a586380519883069c6",
            "f004a88f48d24566b1c644efcc10364a",
            "5aec0095c0f44e38899a5d99acca4096",
            "1526c5978d4e4a489cd0191d59417495",
            "a98b072d3421412cbb644435b9f5e48d",
            "fada2931ac934409b5f64899998ef4a4",
            "aec6be9178604557aa5b843f7fc086c0",
            "972e9039607b4341aeb9e9f19ab91f27",
            "7b4829d844e343de885787c4ac9648c6",
            "c3386e3f0ea64091b861565f926c4836",
            "be17ff37b08b47cb9d8f2b61c28ec23a",
            "4e30dc59f41c40ff8bb445a3a80c1d71",
            "a36d7adc5d094788bdd1a935965e0914",
            "a4d1632070554c089cf9620c38989f38",
            "122fbb42b41c446984b870ee62b5e46c",
            "f6096429b14a469fb9978523211e003c",
            "2386a1035d7a45ec80cf484dadf540eb",
            "fd746f62ae35402d8b8db5a7b2f7d6b2",
            "835c75f2a6e24113a46e67927c8280bf",
            "67c6bee9d7e94aaaad34417417d86c6d",
            "3cd158d8b48141ba9b631d686cb0048a",
            "90d13d7f3a34483b9d1f9790144b1ccf",
            "be0ad0e6507941999ca6e1b6c406b370",
            "3d2f69caaab341799d781777d5467706"
          ]
        },
        "id": "cp9LLKo6NThg",
        "outputId": "694756a1-be2d-4bc4-cfe3-3ba5c12aa4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2f9d456759d41d282803e84d8e334b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89f8474636ce46679e734264609a4bac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aec0095c0f44e38899a5d99acca4096"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4d1632070554c089cf9620c38989f38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Head (classifier layer): Linear(in_features=768, out_features=2, bias=True)\n",
            "Model Output (logits): tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuYli1y-ObCc",
        "outputId": "d11a6ce5-e857-467d-ff98-6272ef6e52e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForSequenceClassification(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): DistilBertSdpaAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the model head consists of:\n",
        "\n",
        "*\tpre_classifier: A linear transformation that prepares hidden states.\n",
        "*\tclassifier: The final layer (this is the main classification head).\n",
        "*\tdropout: A dropout layer to prevent overfitting."
      ],
      "metadata": {
        "id": "njfkVVDiOlPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can directly access the classification head:\n",
        "print(model.classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hA1r2ltOwjW",
        "outputId": "1c4513d5-0076-4307-fc38-8ae8a2b8c806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=768, out_features=2, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means:\n",
        "*\tThe input dimension (in_features=768) corresponds to the hidden size of DistilBERT.\n",
        "* The output dimension (out_features=2) corresponds to the number of classes (positive/negative).\n",
        "* This is the model head, which converts hidden states into class logits."
      ],
      "metadata": {
        "id": "cb175D01O5xL"
      }
    }
  ]
}